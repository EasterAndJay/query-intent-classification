{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the google word2vec model\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674735069275), ('princess', 0.5902431011199951), ('crown_prince', 0.549946129322052), ('prince', 0.5377321243286133)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('week', 0.5004594922065735)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['weekend'], negative=['fun'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Donald_Trump', 0.8103919625282288)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['Trump'],  topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952, 300)\n",
      "(5952,)\n"
     ]
    }
   ],
   "source": [
    "vec_length = model.word_vec('serfdom').shape\n",
    "\n",
    "feature_vecs = []\n",
    "labels = []\n",
    "with open('data/TREC.train') as f:\n",
    "    for line in f:\n",
    "        query_vec = np.zeros(vec_length, dtype='float64')\n",
    "        label, query = line.split()[0], line.split()[1:]\n",
    "        labels.append(int(label))\n",
    "        for word in query:\n",
    "            if word in stop_words.ENGLISH_STOP_WORDS: # ['What', 'How', 'Where', 'Who', 'When', 'Which']:\n",
    "                continue\n",
    "            try:\n",
    "#               summing all wordvecs to get queryvec\n",
    "                query_vec += model.word_vec(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        feature_vecs.append(query_vec)\n",
    "\n",
    "with open('data/TREC.test') as f:\n",
    "    for line in f:\n",
    "        query_vec = np.zeros(vec_length, dtype='float64')\n",
    "        label, query = line.split()[0], line.split()[1:]\n",
    "        labels.append(int(label))\n",
    "        for word in query:\n",
    "            if word in stop_words.ENGLISH_STOP_WORDS: # ['What', 'How', 'Where', 'Who', 'When', 'Which']:\n",
    "                continue\n",
    "            try:\n",
    "#               summing all wordvecs to get queryvec\n",
    "                query_vec += model.word_vec(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        feature_vecs.append(query_vec)\n",
    "\n",
    "feature_vecs = np.asarray(feature_vecs)\n",
    "labels = np.asarray(labels)\n",
    "print(feature_vecs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (5356, 300)\n",
      "Training Labels Shape: (5356,)\n",
      "Testing Features Shape: (596, 300)\n",
      "Testing Labels Shape: (596,)\n"
     ]
    }
   ],
   "source": [
    "(train_features,\n",
    "test_features,\n",
    "train_labels,\n",
    "test_labels) = train_test_split(feature_vecs,\n",
    "                               labels,\n",
    "                               test_size = 0.10,\n",
    "                               random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest = 71% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with n = 1 trees: 0.6073825503355704\n",
      "Error with n = 10 trees: 0.4714765100671141\n",
      "Error with n = 100 trees: 0.29194630872483224\n",
      "Error with n = 500 trees: 0.28859060402684567\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with n decision trees\n",
    "for n in [1,10,100, 500]:\n",
    "    rf = RandomForestClassifier(n_estimators = n, random_state = 42, n_jobs=-1)\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels);\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = rf.predict(test_features)\n",
    "    results = test_labels - predictions\n",
    "    error = results[results != 0].size/results.size\n",
    "    print('Error with n = {} trees: {}'.format(n,error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Regression Trees = 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with n = 1 trees: 0.5251677852348994\n",
      "Error with n = 10 trees: 0.436241610738255\n",
      "Error with n = 100 trees: 0.26677852348993286\n",
      "Error with n = 500 trees: 0.2214765100671141\n"
     ]
    }
   ],
   "source": [
    "for n in [1,10,100, 500]:\n",
    "        gbrt = GradientBoostingClassifier(n_estimators=n, max_depth=3, criterion='mse')\n",
    "        gbrt.fit(train_features, train_labels)\n",
    "        predictions = gbrt.predict(test_features)\n",
    "        results = test_labels - predictions\n",
    "        error = results[results != 0].size/results.size\n",
    "        print('Error with n = {} trees: {}'.format(n,error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers with SGD 79% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, Logistic Regression, Quadratic SVM, Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 = MSE, L1 = Absolute Value of errors, ElasticNet = Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing all possible combinations of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using hinge with none penality, learning rate 1.0 and n = 1 epochs: 0.49328859060402686\n",
      "Error using hinge with none penality, learning rate 1.0 and n = 10 epochs: 0.47315436241610737\n",
      "Error using hinge with none penality, learning rate 1.0 and n = 100 epochs: 0.44798657718120805\n",
      "Error using hinge with none penality, learning rate 1.0 and n = 500 epochs: 0.41946308724832215\n",
      "Error using hinge with none penality, learning rate 1.0 and n = 1000 epochs: 0.41946308724832215\n",
      "Error using hinge with none penality, learning rate 0.1 and n = 1 epochs: 0.40939597315436244\n",
      "Error using hinge with none penality, learning rate 0.1 and n = 10 epochs: 0.36577181208053694\n",
      "Error using hinge with none penality, learning rate 0.1 and n = 100 epochs: 0.3271812080536913\n",
      "Error using hinge with none penality, learning rate 0.1 and n = 500 epochs: 0.3187919463087248\n",
      "Error using hinge with none penality, learning rate 0.1 and n = 1000 epochs: 0.3187919463087248\n",
      "Error using hinge with none penality, learning rate 0.01 and n = 1 epochs: 0.3640939597315436\n",
      "Error using hinge with none penality, learning rate 0.01 and n = 10 epochs: 0.3238255033557047\n",
      "Error using hinge with none penality, learning rate 0.01 and n = 100 epochs: 0.30201342281879195\n",
      "Error using hinge with none penality, learning rate 0.01 and n = 500 epochs: 0.2953020134228188\n",
      "Error using hinge with none penality, learning rate 0.01 and n = 1000 epochs: 0.2902684563758389\n",
      "Error using hinge with none penality, learning rate 0.001 and n = 1 epochs: 0.2953020134228188\n",
      "Error using hinge with none penality, learning rate 0.001 and n = 10 epochs: 0.26174496644295303\n",
      "Error using hinge with none penality, learning rate 0.001 and n = 100 epochs: 0.2600671140939597\n",
      "Error using hinge with none penality, learning rate 0.001 and n = 500 epochs: 0.2751677852348993\n",
      "Error using hinge with none penality, learning rate 0.001 and n = 1000 epochs: 0.26677852348993286\n",
      "Error using hinge with l2 penality, learning rate 1.0 and n = 1 epochs: 0.37919463087248323\n",
      "Error using hinge with l2 penality, learning rate 1.0 and n = 10 epochs: 0.348993288590604\n",
      "Error using hinge with l2 penality, learning rate 1.0 and n = 100 epochs: 0.3338926174496644\n",
      "Error using hinge with l2 penality, learning rate 1.0 and n = 500 epochs: 0.3288590604026846\n",
      "Error using hinge with l2 penality, learning rate 1.0 and n = 1000 epochs: 0.3271812080536913\n",
      "Error using hinge with l2 penality, learning rate 0.1 and n = 1 epochs: 0.28523489932885904\n",
      "Error using hinge with l2 penality, learning rate 0.1 and n = 10 epochs: 0.2802013422818792\n",
      "Error using hinge with l2 penality, learning rate 0.1 and n = 100 epochs: 0.26677852348993286\n",
      "Error using hinge with l2 penality, learning rate 0.1 and n = 500 epochs: 0.2651006711409396\n",
      "Error using hinge with l2 penality, learning rate 0.1 and n = 1000 epochs: 0.26677852348993286\n",
      "Error using hinge with l2 penality, learning rate 0.01 and n = 1 epochs: 0.2785234899328859\n",
      "Error using hinge with l2 penality, learning rate 0.01 and n = 10 epochs: 0.22986577181208054\n",
      "Error using hinge with l2 penality, learning rate 0.01 and n = 100 epochs: 0.2197986577181208\n",
      "Error using hinge with l2 penality, learning rate 0.01 and n = 500 epochs: 0.2181208053691275\n",
      "Error using hinge with l2 penality, learning rate 0.01 and n = 1000 epochs: 0.2214765100671141\n",
      "Error using hinge with l2 penality, learning rate 0.001 and n = 1 epochs: 0.3238255033557047\n",
      "Error using hinge with l2 penality, learning rate 0.001 and n = 10 epochs: 0.2550335570469799\n",
      "Error using hinge with l2 penality, learning rate 0.001 and n = 100 epochs: 0.22483221476510068\n",
      "Error using hinge with l2 penality, learning rate 0.001 and n = 500 epochs: 0.21476510067114093\n",
      "Error using hinge with l2 penality, learning rate 0.001 and n = 1000 epochs: 0.21476510067114093\n",
      "Error using hinge with l1 penality, learning rate 1.0 and n = 1 epochs: 0.7885906040268457\n",
      "Error using hinge with l1 penality, learning rate 1.0 and n = 10 epochs: 0.7885906040268457\n",
      "Error using hinge with l1 penality, learning rate 1.0 and n = 100 epochs: 0.8271812080536913\n",
      "Error using hinge with l1 penality, learning rate 1.0 and n = 500 epochs: 0.7885906040268457\n",
      "Error using hinge with l1 penality, learning rate 1.0 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using hinge with l1 penality, learning rate 0.1 and n = 1 epochs: 0.7885906040268457\n",
      "Error using hinge with l1 penality, learning rate 0.1 and n = 10 epochs: 0.8154362416107382\n",
      "Error using hinge with l1 penality, learning rate 0.1 and n = 100 epochs: 0.8271812080536913\n",
      "Error using hinge with l1 penality, learning rate 0.1 and n = 500 epochs: 0.7885906040268457\n",
      "Error using hinge with l1 penality, learning rate 0.1 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using hinge with l1 penality, learning rate 0.01 and n = 1 epochs: 0.37919463087248323\n",
      "Error using hinge with l1 penality, learning rate 0.01 and n = 10 epochs: 0.43791946308724833\n",
      "Error using hinge with l1 penality, learning rate 0.01 and n = 100 epochs: 0.4463087248322148\n",
      "Error using hinge with l1 penality, learning rate 0.01 and n = 500 epochs: 0.42953020134228187\n",
      "Error using hinge with l1 penality, learning rate 0.01 and n = 1000 epochs: 0.4513422818791946\n",
      "Error using hinge with l1 penality, learning rate 0.001 and n = 1 epochs: 0.31711409395973156\n",
      "Error using hinge with l1 penality, learning rate 0.001 and n = 10 epochs: 0.27684563758389263\n",
      "Error using hinge with l1 penality, learning rate 0.001 and n = 100 epochs: 0.23825503355704697\n",
      "Error using hinge with l1 penality, learning rate 0.001 and n = 500 epochs: 0.22986577181208054\n",
      "Error using hinge with l1 penality, learning rate 0.001 and n = 1000 epochs: 0.23154362416107382\n",
      "Error using hinge with elasticnet penality, learning rate 1.0 and n = 1 epochs: 0.7885906040268457\n",
      "Error using hinge with elasticnet penality, learning rate 1.0 and n = 10 epochs: 0.7885906040268457\n",
      "Error using hinge with elasticnet penality, learning rate 1.0 and n = 100 epochs: 0.8271812080536913\n",
      "Error using hinge with elasticnet penality, learning rate 1.0 and n = 500 epochs: 0.7885906040268457\n",
      "Error using hinge with elasticnet penality, learning rate 1.0 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using hinge with elasticnet penality, learning rate 0.1 and n = 1 epochs: 0.5671140939597316\n",
      "Error using hinge with elasticnet penality, learning rate 0.1 and n = 10 epochs: 0.5100671140939598\n",
      "Error using hinge with elasticnet penality, learning rate 0.1 and n = 100 epochs: 0.6174496644295302\n",
      "Error using hinge with elasticnet penality, learning rate 0.1 and n = 500 epochs: 0.575503355704698\n",
      "Error using hinge with elasticnet penality, learning rate 0.1 and n = 1000 epochs: 0.5083892617449665\n",
      "Error using hinge with elasticnet penality, learning rate 0.01 and n = 1 epochs: 0.25671140939597314\n",
      "Error using hinge with elasticnet penality, learning rate 0.01 and n = 10 epochs: 0.25671140939597314\n",
      "Error using hinge with elasticnet penality, learning rate 0.01 and n = 100 epochs: 0.23993288590604026\n",
      "Error using hinge with elasticnet penality, learning rate 0.01 and n = 500 epochs: 0.23993288590604026\n",
      "Error using hinge with elasticnet penality, learning rate 0.01 and n = 1000 epochs: 0.23993288590604026\n",
      "Error using hinge with elasticnet penality, learning rate 0.001 and n = 1 epochs: 0.28691275167785235\n",
      "Error using hinge with elasticnet penality, learning rate 0.001 and n = 10 epochs: 0.2483221476510067\n",
      "Error using hinge with elasticnet penality, learning rate 0.001 and n = 100 epochs: 0.2197986577181208\n",
      "Error using hinge with elasticnet penality, learning rate 0.001 and n = 500 epochs: 0.2214765100671141\n",
      "Error using hinge with elasticnet penality, learning rate 0.001 and n = 1000 epochs: 0.22651006711409397\n",
      "Error using log with none penality, learning rate 1.0 and n = 1 epochs: 0.46308724832214765\n",
      "Error using log with none penality, learning rate 1.0 and n = 10 epochs: 0.436241610738255\n",
      "Error using log with none penality, learning rate 1.0 and n = 100 epochs: 0.41778523489932884\n",
      "Error using log with none penality, learning rate 1.0 and n = 500 epochs: 0.39429530201342283\n",
      "Error using log with none penality, learning rate 1.0 and n = 1000 epochs: 0.3859060402684564\n",
      "Error using log with none penality, learning rate 0.1 and n = 1 epochs: 0.4228187919463087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using log with none penality, learning rate 0.1 and n = 10 epochs: 0.3808724832214765\n",
      "Error using log with none penality, learning rate 0.1 and n = 100 epochs: 0.34731543624161076\n",
      "Error using log with none penality, learning rate 0.1 and n = 500 epochs: 0.33053691275167785\n",
      "Error using log with none penality, learning rate 0.1 and n = 1000 epochs: 0.3271812080536913\n",
      "Error using log with none penality, learning rate 0.01 and n = 1 epochs: 0.3640939597315436\n",
      "Error using log with none penality, learning rate 0.01 and n = 10 epochs: 0.32046979865771813\n",
      "Error using log with none penality, learning rate 0.01 and n = 100 epochs: 0.30033557046979864\n",
      "Error using log with none penality, learning rate 0.01 and n = 500 epochs: 0.29194630872483224\n",
      "Error using log with none penality, learning rate 0.01 and n = 1000 epochs: 0.28691275167785235\n",
      "Error using log with none penality, learning rate 0.001 and n = 1 epochs: 0.2986577181208054\n",
      "Error using log with none penality, learning rate 0.001 and n = 10 epochs: 0.27348993288590606\n",
      "Error using log with none penality, learning rate 0.001 and n = 100 epochs: 0.27348993288590606\n",
      "Error using log with none penality, learning rate 0.001 and n = 500 epochs: 0.2785234899328859\n",
      "Error using log with none penality, learning rate 0.001 and n = 1000 epochs: 0.25838926174496646\n",
      "Error using log with l2 penality, learning rate 1.0 and n = 1 epochs: 0.44966442953020136\n",
      "Error using log with l2 penality, learning rate 1.0 and n = 10 epochs: 0.40939597315436244\n",
      "Error using log with l2 penality, learning rate 1.0 and n = 100 epochs: 0.40436241610738255\n",
      "Error using log with l2 penality, learning rate 1.0 and n = 500 epochs: 0.40436241610738255\n",
      "Error using log with l2 penality, learning rate 1.0 and n = 1000 epochs: 0.40604026845637586\n",
      "Error using log with l2 penality, learning rate 0.1 and n = 1 epochs: 0.29194630872483224\n",
      "Error using log with l2 penality, learning rate 0.1 and n = 10 epochs: 0.2785234899328859\n",
      "Error using log with l2 penality, learning rate 0.1 and n = 100 epochs: 0.2902684563758389\n",
      "Error using log with l2 penality, learning rate 0.1 and n = 500 epochs: 0.2936241610738255\n",
      "Error using log with l2 penality, learning rate 0.1 and n = 1000 epochs: 0.29194630872483224\n",
      "Error using log with l2 penality, learning rate 0.01 and n = 1 epochs: 0.2516778523489933\n",
      "Error using log with l2 penality, learning rate 0.01 and n = 10 epochs: 0.22986577181208054\n",
      "Error using log with l2 penality, learning rate 0.01 and n = 100 epochs: 0.22651006711409397\n",
      "Error using log with l2 penality, learning rate 0.01 and n = 500 epochs: 0.22651006711409397\n",
      "Error using log with l2 penality, learning rate 0.01 and n = 1000 epochs: 0.22651006711409397\n",
      "Error using log with l2 penality, learning rate 0.001 and n = 1 epochs: 0.29697986577181207\n",
      "Error using log with l2 penality, learning rate 0.001 and n = 10 epochs: 0.23657718120805368\n",
      "Error using log with l2 penality, learning rate 0.001 and n = 100 epochs: 0.22818791946308725\n",
      "Error using log with l2 penality, learning rate 0.001 and n = 500 epochs: 0.22651006711409397\n",
      "Error using log with l2 penality, learning rate 0.001 and n = 1000 epochs: 0.2231543624161074\n",
      "Error using log with l1 penality, learning rate 1.0 and n = 1 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 1.0 and n = 10 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 1.0 and n = 100 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 1.0 and n = 500 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 1.0 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 0.1 and n = 1 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 0.1 and n = 10 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 0.1 and n = 100 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 0.1 and n = 500 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 0.1 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using log with l1 penality, learning rate 0.01 and n = 1 epochs: 0.40268456375838924\n",
      "Error using log with l1 penality, learning rate 0.01 and n = 10 epochs: 0.3691275167785235\n",
      "Error using log with l1 penality, learning rate 0.01 and n = 100 epochs: 0.348993288590604\n",
      "Error using log with l1 penality, learning rate 0.01 and n = 500 epochs: 0.35570469798657717\n",
      "Error using log with l1 penality, learning rate 0.01 and n = 1000 epochs: 0.3540268456375839\n",
      "Error using log with l1 penality, learning rate 0.001 and n = 1 epochs: 0.3087248322147651\n",
      "Error using log with l1 penality, learning rate 0.001 and n = 10 epochs: 0.3036912751677852\n",
      "Error using log with l1 penality, learning rate 0.001 and n = 100 epochs: 0.2516778523489933\n",
      "Error using log with l1 penality, learning rate 0.001 and n = 500 epochs: 0.23657718120805368\n",
      "Error using log with l1 penality, learning rate 0.001 and n = 1000 epochs: 0.23825503355704697\n",
      "Error using log with elasticnet penality, learning rate 1.0 and n = 1 epochs: 0.7684563758389261\n",
      "Error using log with elasticnet penality, learning rate 1.0 and n = 10 epochs: 0.7684563758389261\n",
      "Error using log with elasticnet penality, learning rate 1.0 and n = 100 epochs: 0.7684563758389261\n",
      "Error using log with elasticnet penality, learning rate 1.0 and n = 500 epochs: 0.7684563758389261\n",
      "Error using log with elasticnet penality, learning rate 1.0 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using log with elasticnet penality, learning rate 0.1 and n = 1 epochs: 0.5167785234899329\n",
      "Error using log with elasticnet penality, learning rate 0.1 and n = 10 epochs: 0.4848993288590604\n",
      "Error using log with elasticnet penality, learning rate 0.1 and n = 100 epochs: 0.4848993288590604\n",
      "Error using log with elasticnet penality, learning rate 0.1 and n = 500 epochs: 0.4848993288590604\n",
      "Error using log with elasticnet penality, learning rate 0.1 and n = 1000 epochs: 0.4815436241610738\n",
      "Error using log with elasticnet penality, learning rate 0.01 and n = 1 epochs: 0.28187919463087246\n",
      "Error using log with elasticnet penality, learning rate 0.01 and n = 10 epochs: 0.25671140939597314\n",
      "Error using log with elasticnet penality, learning rate 0.01 and n = 100 epochs: 0.2550335570469799\n",
      "Error using log with elasticnet penality, learning rate 0.01 and n = 500 epochs: 0.2516778523489933\n",
      "Error using log with elasticnet penality, learning rate 0.01 and n = 1000 epochs: 0.25\n",
      "Error using log with elasticnet penality, learning rate 0.001 and n = 1 epochs: 0.2802013422818792\n",
      "Error using log with elasticnet penality, learning rate 0.001 and n = 10 epochs: 0.2332214765100671\n",
      "Error using log with elasticnet penality, learning rate 0.001 and n = 100 epochs: 0.2332214765100671\n",
      "Error using log with elasticnet penality, learning rate 0.001 and n = 500 epochs: 0.2231543624161074\n",
      "Error using log with elasticnet penality, learning rate 0.001 and n = 1000 epochs: 0.2214765100671141\n",
      "Error using squared_hinge with none penality, learning rate 1.0 and n = 1 epochs: 0.5503355704697986\n",
      "Error using squared_hinge with none penality, learning rate 1.0 and n = 10 epochs: 0.5167785234899329\n",
      "Error using squared_hinge with none penality, learning rate 1.0 and n = 100 epochs: 0.4983221476510067\n",
      "Error using squared_hinge with none penality, learning rate 1.0 and n = 500 epochs: 0.4983221476510067\n",
      "Error using squared_hinge with none penality, learning rate 1.0 and n = 1000 epochs: 0.4949664429530201\n",
      "Error using squared_hinge with none penality, learning rate 0.1 and n = 1 epochs: 0.46140939597315433\n",
      "Error using squared_hinge with none penality, learning rate 0.1 and n = 10 epochs: 0.4261744966442953\n",
      "Error using squared_hinge with none penality, learning rate 0.1 and n = 100 epochs: 0.41442953020134227\n",
      "Error using squared_hinge with none penality, learning rate 0.1 and n = 500 epochs: 0.3959731543624161\n",
      "Error using squared_hinge with none penality, learning rate 0.1 and n = 1000 epochs: 0.3875838926174497\n",
      "Error using squared_hinge with none penality, learning rate 0.01 and n = 1 epochs: 0.35906040268456374\n",
      "Error using squared_hinge with none penality, learning rate 0.01 and n = 10 epochs: 0.31711409395973156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using squared_hinge with none penality, learning rate 0.01 and n = 100 epochs: 0.3053691275167785\n",
      "Error using squared_hinge with none penality, learning rate 0.01 and n = 500 epochs: 0.2986577181208054\n",
      "Error using squared_hinge with none penality, learning rate 0.01 and n = 1000 epochs: 0.30033557046979864\n",
      "Error using squared_hinge with none penality, learning rate 0.001 and n = 1 epochs: 0.28187919463087246\n",
      "Error using squared_hinge with none penality, learning rate 0.001 and n = 10 epochs: 0.2516778523489933\n",
      "Error using squared_hinge with none penality, learning rate 0.001 and n = 100 epochs: 0.27348993288590606\n",
      "Error using squared_hinge with none penality, learning rate 0.001 and n = 500 epochs: 0.337248322147651\n",
      "Error using squared_hinge with none penality, learning rate 0.001 and n = 1000 epochs: 0.34060402684563756\n",
      "Error using squared_hinge with l2 penality, learning rate 1.0 and n = 1 epochs: 0.7298657718120806\n",
      "Error using squared_hinge with l2 penality, learning rate 1.0 and n = 10 epochs: 0.7583892617449665\n",
      "Error using squared_hinge with l2 penality, learning rate 1.0 and n = 100 epochs: 0.7634228187919463\n",
      "Error using squared_hinge with l2 penality, learning rate 1.0 and n = 500 epochs: 0.7634228187919463\n",
      "Error using squared_hinge with l2 penality, learning rate 1.0 and n = 1000 epochs: 0.7634228187919463\n",
      "Error using squared_hinge with l2 penality, learning rate 0.1 and n = 1 epochs: 0.4446308724832215\n",
      "Error using squared_hinge with l2 penality, learning rate 0.1 and n = 10 epochs: 0.5016778523489933\n",
      "Error using squared_hinge with l2 penality, learning rate 0.1 and n = 100 epochs: 0.5201342281879194\n",
      "Error using squared_hinge with l2 penality, learning rate 0.1 and n = 500 epochs: 0.5167785234899329\n",
      "Error using squared_hinge with l2 penality, learning rate 0.1 and n = 1000 epochs: 0.5167785234899329\n",
      "Error using squared_hinge with l2 penality, learning rate 0.01 and n = 1 epochs: 0.33557046979865773\n",
      "Error using squared_hinge with l2 penality, learning rate 0.01 and n = 10 epochs: 0.38422818791946306\n",
      "Error using squared_hinge with l2 penality, learning rate 0.01 and n = 100 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with l2 penality, learning rate 0.01 and n = 500 epochs: 0.41946308724832215\n",
      "Error using squared_hinge with l2 penality, learning rate 0.01 and n = 1000 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with l2 penality, learning rate 0.001 and n = 1 epochs: 0.3036912751677852\n",
      "Error using squared_hinge with l2 penality, learning rate 0.001 and n = 10 epochs: 0.3422818791946309\n",
      "Error using squared_hinge with l2 penality, learning rate 0.001 and n = 100 epochs: 0.3825503355704698\n",
      "Error using squared_hinge with l2 penality, learning rate 0.001 and n = 500 epochs: 0.4228187919463087\n",
      "Error using squared_hinge with l2 penality, learning rate 0.001 and n = 1000 epochs: 0.3724832214765101\n",
      "Error using squared_hinge with l1 penality, learning rate 1.0 and n = 1 epochs: 0.6426174496644296\n",
      "Error using squared_hinge with l1 penality, learning rate 1.0 and n = 10 epochs: 0.6325503355704698\n",
      "Error using squared_hinge with l1 penality, learning rate 1.0 and n = 100 epochs: 0.6342281879194631\n",
      "Error using squared_hinge with l1 penality, learning rate 1.0 and n = 500 epochs: 0.62751677852349\n",
      "Error using squared_hinge with l1 penality, learning rate 1.0 and n = 1000 epochs: 0.6291946308724832\n",
      "Error using squared_hinge with l1 penality, learning rate 0.1 and n = 1 epochs: 0.47315436241610737\n",
      "Error using squared_hinge with l1 penality, learning rate 0.1 and n = 10 epochs: 0.4395973154362416\n",
      "Error using squared_hinge with l1 penality, learning rate 0.1 and n = 100 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with l1 penality, learning rate 0.1 and n = 500 epochs: 0.4077181208053691\n",
      "Error using squared_hinge with l1 penality, learning rate 0.1 and n = 1000 epochs: 0.40436241610738255\n",
      "Error using squared_hinge with l1 penality, learning rate 0.01 and n = 1 epochs: 0.3624161073825503\n",
      "Error using squared_hinge with l1 penality, learning rate 0.01 and n = 10 epochs: 0.3288590604026846\n",
      "Error using squared_hinge with l1 penality, learning rate 0.01 and n = 100 epochs: 0.31208053691275167\n",
      "Error using squared_hinge with l1 penality, learning rate 0.01 and n = 500 epochs: 0.29194630872483224\n",
      "Error using squared_hinge with l1 penality, learning rate 0.01 and n = 1000 epochs: 0.29194630872483224\n",
      "Error using squared_hinge with l1 penality, learning rate 0.001 and n = 1 epochs: 0.28523489932885904\n",
      "Error using squared_hinge with l1 penality, learning rate 0.001 and n = 10 epochs: 0.2785234899328859\n",
      "Error using squared_hinge with l1 penality, learning rate 0.001 and n = 100 epochs: 0.29194630872483224\n",
      "Error using squared_hinge with l1 penality, learning rate 0.001 and n = 500 epochs: 0.348993288590604\n",
      "Error using squared_hinge with l1 penality, learning rate 0.001 and n = 1000 epochs: 0.348993288590604\n",
      "Error using squared_hinge with elasticnet penality, learning rate 1.0 and n = 1 epochs: 0.6929530201342282\n",
      "Error using squared_hinge with elasticnet penality, learning rate 1.0 and n = 10 epochs: 0.7399328859060402\n",
      "Error using squared_hinge with elasticnet penality, learning rate 1.0 and n = 100 epochs: 0.7567114093959731\n",
      "Error using squared_hinge with elasticnet penality, learning rate 1.0 and n = 500 epochs: 0.7583892617449665\n",
      "Error using squared_hinge with elasticnet penality, learning rate 1.0 and n = 1000 epochs: 0.7583892617449665\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.1 and n = 1 epochs: 0.4312080536912752\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.1 and n = 10 epochs: 0.48322147651006714\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.1 and n = 100 epochs: 0.5167785234899329\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.1 and n = 500 epochs: 0.5251677852348994\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.1 and n = 1000 epochs: 0.5251677852348994\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.01 and n = 1 epochs: 0.3271812080536913\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.01 and n = 10 epochs: 0.3674496644295302\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.01 and n = 100 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.01 and n = 500 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.01 and n = 1000 epochs: 0.4161073825503356\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.001 and n = 1 epochs: 0.28691275167785235\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.001 and n = 10 epochs: 0.3288590604026846\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.001 and n = 100 epochs: 0.37919463087248323\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.001 and n = 500 epochs: 0.4278523489932886\n",
      "Error using squared_hinge with elasticnet penality, learning rate 0.001 and n = 1000 epochs: 0.37080536912751677\n",
      "Error using perceptron with none penality, learning rate 1.0 and n = 1 epochs: 0.511744966442953\n",
      "Error using perceptron with none penality, learning rate 1.0 and n = 10 epochs: 0.4697986577181208\n",
      "Error using perceptron with none penality, learning rate 1.0 and n = 100 epochs: 0.45805369127516776\n",
      "Error using perceptron with none penality, learning rate 1.0 and n = 500 epochs: 0.4429530201342282\n",
      "Error using perceptron with none penality, learning rate 1.0 and n = 1000 epochs: 0.4429530201342282\n",
      "Error using perceptron with none penality, learning rate 0.1 and n = 1 epochs: 0.412751677852349\n",
      "Error using perceptron with none penality, learning rate 0.1 and n = 10 epochs: 0.3775167785234899\n",
      "Error using perceptron with none penality, learning rate 0.1 and n = 100 epochs: 0.348993288590604\n",
      "Error using perceptron with none penality, learning rate 0.1 and n = 500 epochs: 0.34060402684563756\n",
      "Error using perceptron with none penality, learning rate 0.1 and n = 1000 epochs: 0.3389261744966443\n",
      "Error using perceptron with none penality, learning rate 0.01 and n = 1 epochs: 0.37416107382550334\n",
      "Error using perceptron with none penality, learning rate 0.01 and n = 10 epochs: 0.31711409395973156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using perceptron with none penality, learning rate 0.01 and n = 100 epochs: 0.3070469798657718\n",
      "Error using perceptron with none penality, learning rate 0.01 and n = 500 epochs: 0.3053691275167785\n",
      "Error using perceptron with none penality, learning rate 0.01 and n = 1000 epochs: 0.30033557046979864\n",
      "Error using perceptron with none penality, learning rate 0.001 and n = 1 epochs: 0.2986577181208054\n",
      "Error using perceptron with none penality, learning rate 0.001 and n = 10 epochs: 0.2634228187919463\n",
      "Error using perceptron with none penality, learning rate 0.001 and n = 100 epochs: 0.26677852348993286\n",
      "Error using perceptron with none penality, learning rate 0.001 and n = 500 epochs: 0.28859060402684567\n",
      "Error using perceptron with none penality, learning rate 0.001 and n = 1000 epochs: 0.32550335570469796\n",
      "Error using perceptron with l2 penality, learning rate 1.0 and n = 1 epochs: 0.5067114093959731\n",
      "Error using perceptron with l2 penality, learning rate 1.0 and n = 10 epochs: 0.49161073825503354\n",
      "Error using perceptron with l2 penality, learning rate 1.0 and n = 100 epochs: 0.4697986577181208\n",
      "Error using perceptron with l2 penality, learning rate 1.0 and n = 500 epochs: 0.48825503355704697\n",
      "Error using perceptron with l2 penality, learning rate 1.0 and n = 1000 epochs: 0.47818791946308725\n",
      "Error using perceptron with l2 penality, learning rate 0.1 and n = 1 epochs: 0.34731543624161076\n",
      "Error using perceptron with l2 penality, learning rate 0.1 and n = 10 epochs: 0.3187919463087248\n",
      "Error using perceptron with l2 penality, learning rate 0.1 and n = 100 epochs: 0.3187919463087248\n",
      "Error using perceptron with l2 penality, learning rate 0.1 and n = 500 epochs: 0.34395973154362414\n",
      "Error using perceptron with l2 penality, learning rate 0.1 and n = 1000 epochs: 0.35067114093959734\n",
      "Error using perceptron with l2 penality, learning rate 0.01 and n = 1 epochs: 0.3070469798657718\n",
      "Error using perceptron with l2 penality, learning rate 0.01 and n = 10 epochs: 0.30201342281879195\n",
      "Error using perceptron with l2 penality, learning rate 0.01 and n = 100 epochs: 0.3070469798657718\n",
      "Error using perceptron with l2 penality, learning rate 0.01 and n = 500 epochs: 0.33557046979865773\n",
      "Error using perceptron with l2 penality, learning rate 0.01 and n = 1000 epochs: 0.33053691275167785\n",
      "Error using perceptron with l2 penality, learning rate 0.001 and n = 1 epochs: 0.3221476510067114\n",
      "Error using perceptron with l2 penality, learning rate 0.001 and n = 10 epochs: 0.3070469798657718\n",
      "Error using perceptron with l2 penality, learning rate 0.001 and n = 100 epochs: 0.30201342281879195\n",
      "Error using perceptron with l2 penality, learning rate 0.001 and n = 500 epochs: 0.30033557046979864\n",
      "Error using perceptron with l2 penality, learning rate 0.001 and n = 1000 epochs: 0.34395973154362414\n",
      "Error using perceptron with l1 penality, learning rate 1.0 and n = 1 epochs: 0.7885906040268457\n",
      "Error using perceptron with l1 penality, learning rate 1.0 and n = 10 epochs: 0.8154362416107382\n",
      "Error using perceptron with l1 penality, learning rate 1.0 and n = 100 epochs: 0.8271812080536913\n",
      "Error using perceptron with l1 penality, learning rate 1.0 and n = 500 epochs: 0.8154362416107382\n",
      "Error using perceptron with l1 penality, learning rate 1.0 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using perceptron with l1 penality, learning rate 0.1 and n = 1 epochs: 0.7885906040268457\n",
      "Error using perceptron with l1 penality, learning rate 0.1 and n = 10 epochs: 0.8154362416107382\n",
      "Error using perceptron with l1 penality, learning rate 0.1 and n = 100 epochs: 0.8271812080536913\n",
      "Error using perceptron with l1 penality, learning rate 0.1 and n = 500 epochs: 0.8154362416107382\n",
      "Error using perceptron with l1 penality, learning rate 0.1 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using perceptron with l1 penality, learning rate 0.01 and n = 1 epochs: 0.610738255033557\n",
      "Error using perceptron with l1 penality, learning rate 0.01 and n = 10 epochs: 0.6510067114093959\n",
      "Error using perceptron with l1 penality, learning rate 0.01 and n = 100 epochs: 0.6493288590604027\n",
      "Error using perceptron with l1 penality, learning rate 0.01 and n = 500 epochs: 0.6040268456375839\n",
      "Error using perceptron with l1 penality, learning rate 0.01 and n = 1000 epochs: 0.6493288590604027\n",
      "Error using perceptron with l1 penality, learning rate 0.001 and n = 1 epochs: 0.3053691275167785\n",
      "Error using perceptron with l1 penality, learning rate 0.001 and n = 10 epochs: 0.313758389261745\n",
      "Error using perceptron with l1 penality, learning rate 0.001 and n = 100 epochs: 0.4865771812080537\n",
      "Error using perceptron with l1 penality, learning rate 0.001 and n = 500 epochs: 0.3691275167785235\n",
      "Error using perceptron with l1 penality, learning rate 0.001 and n = 1000 epochs: 0.45302013422818793\n",
      "Error using perceptron with elasticnet penality, learning rate 1.0 and n = 1 epochs: 0.7885906040268457\n",
      "Error using perceptron with elasticnet penality, learning rate 1.0 and n = 10 epochs: 0.8154362416107382\n",
      "Error using perceptron with elasticnet penality, learning rate 1.0 and n = 100 epochs: 0.8271812080536913\n",
      "Error using perceptron with elasticnet penality, learning rate 1.0 and n = 500 epochs: 0.8154362416107382\n",
      "Error using perceptron with elasticnet penality, learning rate 1.0 and n = 1000 epochs: 0.7684563758389261\n",
      "Error using perceptron with elasticnet penality, learning rate 0.1 and n = 1 epochs: 0.6761744966442953\n",
      "Error using perceptron with elasticnet penality, learning rate 0.1 and n = 10 epochs: 0.6459731543624161\n",
      "Error using perceptron with elasticnet penality, learning rate 0.1 and n = 100 epochs: 0.6711409395973155\n",
      "Error using perceptron with elasticnet penality, learning rate 0.1 and n = 500 epochs: 0.6476510067114094\n",
      "Error using perceptron with elasticnet penality, learning rate 0.1 and n = 1000 epochs: 0.6208053691275168\n",
      "Error using perceptron with elasticnet penality, learning rate 0.01 and n = 1 epochs: 0.3674496644295302\n",
      "Error using perceptron with elasticnet penality, learning rate 0.01 and n = 10 epochs: 0.41778523489932884\n",
      "Error using perceptron with elasticnet penality, learning rate 0.01 and n = 100 epochs: 0.4412751677852349\n",
      "Error using perceptron with elasticnet penality, learning rate 0.01 and n = 500 epochs: 0.4563758389261745\n",
      "Error using perceptron with elasticnet penality, learning rate 0.01 and n = 1000 epochs: 0.41442953020134227\n",
      "Error using perceptron with elasticnet penality, learning rate 0.001 and n = 1 epochs: 0.2986577181208054\n",
      "Error using perceptron with elasticnet penality, learning rate 0.001 and n = 10 epochs: 0.30201342281879195\n",
      "Error using perceptron with elasticnet penality, learning rate 0.001 and n = 100 epochs: 0.27684563758389263\n",
      "Error using perceptron with elasticnet penality, learning rate 0.001 and n = 500 epochs: 0.3523489932885906\n",
      "Error using perceptron with elasticnet penality, learning rate 0.001 and n = 1000 epochs: 0.3573825503355705\n"
     ]
    }
   ],
   "source": [
    "for alg in ['hinge', 'log', 'squared_hinge', 'perceptron']:\n",
    "    for penalty in ['none', 'l2', 'l1', 'elasticnet']:\n",
    "        for alpha in [1.0, 0.1, 0.01, 0.001]:\n",
    "            for n in [1, 10, 100, 500, 1000]:\n",
    "                sgd = SGDClassifier(loss=alg,\n",
    "                                    penalty=penalty,\n",
    "                                    alpha=alpha, l1_ratio=0.15,\n",
    "                                    fit_intercept=True,\n",
    "                                    max_iter=n,\n",
    "                                    tol=None,\n",
    "                                    shuffle=True,\n",
    "                                    verbose=0,\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=70,\n",
    "                                    learning_rate='optimal',\n",
    "                                    power_t=0.5,\n",
    "                                    warm_start=False,\n",
    "                                    average=False)\n",
    "                sgd.fit(train_features, train_labels)\n",
    "                predictions = sgd.predict(test_features)\n",
    "                results = test_labels - predictions\n",
    "                error = results[results != 0].size/results.size\n",
    "                print('Error using {} with {} penalty, learning rate {} and n = {} epochs: {}'.format(alg, penalty, alpha, n, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test differing learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using hinge with l2 penalty, learning rate 0.01 and n = 100 epochs: 0.2197986577181208\n",
      "Error using hinge with l2 penalty, learning rate 0.001 and n = 100 epochs: 0.22483221476510068\n",
      "Error using hinge with l2 penalty, learning rate 0.01 and n = 500 epochs: 0.2181208053691275\n",
      "Error using hinge with l2 penalty, learning rate 0.001 and n = 500 epochs: 0.21476510067114093\n",
      "Error using hinge with l2 penalty, learning rate 0.01 and n = 1000 epochs: 0.2214765100671141\n",
      "Error using hinge with l2 penalty, learning rate 0.001 and n = 1000 epochs: 0.21476510067114093\n",
      "Error using hinge with l1 penalty, learning rate 0.01 and n = 100 epochs: 0.4463087248322148\n",
      "Error using hinge with l1 penalty, learning rate 0.001 and n = 100 epochs: 0.23825503355704697\n",
      "Error using hinge with l1 penalty, learning rate 0.01 and n = 500 epochs: 0.42953020134228187\n",
      "Error using hinge with l1 penalty, learning rate 0.001 and n = 500 epochs: 0.22986577181208054\n",
      "Error using hinge with l1 penalty, learning rate 0.01 and n = 1000 epochs: 0.4513422818791946\n",
      "Error using hinge with l1 penalty, learning rate 0.001 and n = 1000 epochs: 0.23154362416107382\n",
      "Error using hinge with elasticnet penalty, learning rate 0.01 and n = 100 epochs: 0.23993288590604026\n",
      "Error using hinge with elasticnet penalty, learning rate 0.001 and n = 100 epochs: 0.2197986577181208\n",
      "Error using hinge with elasticnet penalty, learning rate 0.01 and n = 500 epochs: 0.23993288590604026\n",
      "Error using hinge with elasticnet penalty, learning rate 0.001 and n = 500 epochs: 0.2214765100671141\n",
      "Error using hinge with elasticnet penalty, learning rate 0.01 and n = 1000 epochs: 0.23993288590604026\n",
      "Error using hinge with elasticnet penalty, learning rate 0.001 and n = 1000 epochs: 0.22651006711409397\n",
      "Error using log with l2 penalty, learning rate 0.01 and n = 100 epochs: 0.22651006711409397\n",
      "Error using log with l2 penalty, learning rate 0.001 and n = 100 epochs: 0.22818791946308725\n",
      "Error using log with l2 penalty, learning rate 0.01 and n = 500 epochs: 0.22651006711409397\n",
      "Error using log with l2 penalty, learning rate 0.001 and n = 500 epochs: 0.22651006711409397\n",
      "Error using log with l2 penalty, learning rate 0.01 and n = 1000 epochs: 0.22651006711409397\n",
      "Error using log with l2 penalty, learning rate 0.001 and n = 1000 epochs: 0.2231543624161074\n",
      "Error using log with l1 penalty, learning rate 0.01 and n = 100 epochs: 0.348993288590604\n",
      "Error using log with l1 penalty, learning rate 0.001 and n = 100 epochs: 0.2516778523489933\n",
      "Error using log with l1 penalty, learning rate 0.01 and n = 500 epochs: 0.35570469798657717\n",
      "Error using log with l1 penalty, learning rate 0.001 and n = 500 epochs: 0.23657718120805368\n",
      "Error using log with l1 penalty, learning rate 0.01 and n = 1000 epochs: 0.3540268456375839\n",
      "Error using log with l1 penalty, learning rate 0.001 and n = 1000 epochs: 0.23825503355704697\n",
      "Error using log with elasticnet penalty, learning rate 0.01 and n = 100 epochs: 0.2550335570469799\n",
      "Error using log with elasticnet penalty, learning rate 0.001 and n = 100 epochs: 0.2332214765100671\n",
      "Error using log with elasticnet penalty, learning rate 0.01 and n = 500 epochs: 0.2516778523489933\n",
      "Error using log with elasticnet penalty, learning rate 0.001 and n = 500 epochs: 0.2231543624161074\n",
      "Error using log with elasticnet penalty, learning rate 0.01 and n = 1000 epochs: 0.25\n",
      "Error using log with elasticnet penalty, learning rate 0.001 and n = 1000 epochs: 0.2214765100671141\n",
      "Error using squared_hinge with l2 penalty, learning rate 0.01 and n = 100 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with l2 penalty, learning rate 0.001 and n = 100 epochs: 0.3825503355704698\n",
      "Error using squared_hinge with l2 penalty, learning rate 0.01 and n = 500 epochs: 0.41946308724832215\n",
      "Error using squared_hinge with l2 penalty, learning rate 0.001 and n = 500 epochs: 0.4228187919463087\n",
      "Error using squared_hinge with l2 penalty, learning rate 0.01 and n = 1000 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with l2 penalty, learning rate 0.001 and n = 1000 epochs: 0.3724832214765101\n",
      "Error using squared_hinge with l1 penalty, learning rate 0.01 and n = 100 epochs: 0.31208053691275167\n",
      "Error using squared_hinge with l1 penalty, learning rate 0.001 and n = 100 epochs: 0.29194630872483224\n",
      "Error using squared_hinge with l1 penalty, learning rate 0.01 and n = 500 epochs: 0.29194630872483224\n",
      "Error using squared_hinge with l1 penalty, learning rate 0.001 and n = 500 epochs: 0.348993288590604\n",
      "Error using squared_hinge with l1 penalty, learning rate 0.01 and n = 1000 epochs: 0.29194630872483224\n",
      "Error using squared_hinge with l1 penalty, learning rate 0.001 and n = 1000 epochs: 0.348993288590604\n",
      "Error using squared_hinge with elasticnet penalty, learning rate 0.01 and n = 100 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with elasticnet penalty, learning rate 0.001 and n = 100 epochs: 0.37919463087248323\n",
      "Error using squared_hinge with elasticnet penalty, learning rate 0.01 and n = 500 epochs: 0.41778523489932884\n",
      "Error using squared_hinge with elasticnet penalty, learning rate 0.001 and n = 500 epochs: 0.4278523489932886\n",
      "Error using squared_hinge with elasticnet penalty, learning rate 0.01 and n = 1000 epochs: 0.4161073825503356\n",
      "Error using squared_hinge with elasticnet penalty, learning rate 0.001 and n = 1000 epochs: 0.37080536912751677\n",
      "Error using perceptron with l2 penalty, learning rate 0.01 and n = 100 epochs: 0.3070469798657718\n",
      "Error using perceptron with l2 penalty, learning rate 0.001 and n = 100 epochs: 0.30201342281879195\n",
      "Error using perceptron with l2 penalty, learning rate 0.01 and n = 500 epochs: 0.33557046979865773\n",
      "Error using perceptron with l2 penalty, learning rate 0.001 and n = 500 epochs: 0.30033557046979864\n",
      "Error using perceptron with l2 penalty, learning rate 0.01 and n = 1000 epochs: 0.33053691275167785\n",
      "Error using perceptron with l2 penalty, learning rate 0.001 and n = 1000 epochs: 0.34395973154362414\n",
      "Error using perceptron with l1 penalty, learning rate 0.01 and n = 100 epochs: 0.6493288590604027\n",
      "Error using perceptron with l1 penalty, learning rate 0.001 and n = 100 epochs: 0.4865771812080537\n",
      "Error using perceptron with l1 penalty, learning rate 0.01 and n = 500 epochs: 0.6040268456375839\n",
      "Error using perceptron with l1 penalty, learning rate 0.001 and n = 500 epochs: 0.3691275167785235\n",
      "Error using perceptron with l1 penalty, learning rate 0.01 and n = 1000 epochs: 0.6493288590604027\n",
      "Error using perceptron with l1 penalty, learning rate 0.001 and n = 1000 epochs: 0.45302013422818793\n",
      "Error using perceptron with elasticnet penalty, learning rate 0.01 and n = 100 epochs: 0.4412751677852349\n",
      "Error using perceptron with elasticnet penalty, learning rate 0.001 and n = 100 epochs: 0.27684563758389263\n",
      "Error using perceptron with elasticnet penalty, learning rate 0.01 and n = 500 epochs: 0.4563758389261745\n",
      "Error using perceptron with elasticnet penalty, learning rate 0.001 and n = 500 epochs: 0.3523489932885906\n",
      "Error using perceptron with elasticnet penalty, learning rate 0.01 and n = 1000 epochs: 0.41442953020134227\n",
      "Error using perceptron with elasticnet penalty, learning rate 0.001 and n = 1000 epochs: 0.3573825503355705\n"
     ]
    }
   ],
   "source": [
    "for alg in ['hinge', 'log', 'squared_hinge', 'perceptron']:\n",
    "    for penalty in ['l2', 'l1', 'elasticnet']:\n",
    "            for n in [100, 500, 1000]:\n",
    "                for alpha in [.01, .001]:\n",
    "                    sgd = SGDClassifier(loss=alg,\n",
    "                                        penalty=penalty,\n",
    "                                        alpha=alpha, l1_ratio=0.15,\n",
    "                                        fit_intercept=True,\n",
    "                                        max_iter=n,\n",
    "                                        tol=None,\n",
    "                                        shuffle=True,\n",
    "                                        verbose=0,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=70,\n",
    "                                        learning_rate='optimal',\n",
    "                                        power_t=0.5,\n",
    "                                        warm_start=False,\n",
    "                                        average=False)\n",
    "                    sgd.fit(train_features, train_labels)\n",
    "                    predictions = sgd.predict(test_features)\n",
    "                    results = test_labels - predictions\n",
    "                    error = results[results != 0].size/results.size\n",
    "                    print('Error using {} with {} penalty, learning rate {} and n = {} epochs: {}'.format(alg, penalty, alpha, n, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM is best with 79% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using hinge with l2 penalty: 0.21476510067114093\n",
      "Error using hinge with l1 penalty: 0.22986577181208054\n",
      "Error using hinge with elasticnet penalty: 0.2214765100671141\n",
      "Error using log with l2 penalty: 0.22651006711409397\n",
      "Error using log with l1 penalty: 0.23657718120805368\n",
      "Error using log with elasticnet penalty: 0.2231543624161074\n",
      "Error using squared_hinge with l2 penalty: 0.4228187919463087\n",
      "Error using squared_hinge with l1 penalty: 0.348993288590604\n",
      "Error using squared_hinge with elasticnet penalty: 0.4278523489932886\n",
      "Error using perceptron with l2 penalty: 0.30033557046979864\n",
      "Error using perceptron with l1 penalty: 0.3691275167785235\n",
      "Error using perceptron with elasticnet penalty: 0.3523489932885906\n"
     ]
    }
   ],
   "source": [
    "for alg in ['hinge', 'log', 'squared_hinge', 'perceptron']:\n",
    "    for penalty in ['l2', 'l1', 'elasticnet']:\n",
    "        n = 500\n",
    "        alpha = 0.001\n",
    "        sgd = SGDClassifier(loss=alg,\n",
    "                            penalty=penalty,\n",
    "                            alpha=alpha, l1_ratio=0.15,\n",
    "                            fit_intercept=True,\n",
    "                            max_iter=n,\n",
    "                            tol=None,\n",
    "                            shuffle=True,\n",
    "                            verbose=0,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=70,\n",
    "                            learning_rate='optimal',\n",
    "                            power_t=0.5,\n",
    "                            warm_start=False,\n",
    "                            average=False)\n",
    "        sgd.fit(train_features, train_labels)\n",
    "        predictions = sgd.predict(test_features)\n",
    "        results = test_labels - predictions\n",
    "        error = results[results != 0].size/results.size\n",
    "        print('Error using {} with {} penalty: {}'.format(alg, penalty, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - 80% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using n = 1000 epochs: 0.2063758389261745\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(100, ),\n",
    "                                activation='relu',\n",
    "                                alpha=0.001,\n",
    "                                batch_size='auto',\n",
    "                                learning_rate='constant',\n",
    "                                learning_rate_init=0.0001,\n",
    "                                power_t=0.5,\n",
    "                                max_iter=1000,\n",
    "                                shuffle=True,\n",
    "                                random_state=40,\n",
    "                                verbose=False)\n",
    "nn.fit(train_features, train_labels)\n",
    "predictions = nn.predict(test_features)\n",
    "results = test_labels - predictions\n",
    "error = results[results != 0].size/results.size\n",
    "print('Error using n = {} epochs: {}'.format(1000, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
